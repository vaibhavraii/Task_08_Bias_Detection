# ğŸ§  Task_08_Bias_Detection

**Research Task 08 â€” Detecting Bias in LLM-Generated Data Narratives**  
Syracuse University | Faculty Sponsor: Jonathan Stromer-Galley  
**Date:** November 15, 2025  


1. Overview 
This report presents the final submission for the November 15 OPT research requirement. In this project, the student executed a controlled experiment to analyze whether large language models (LLMs) generate biased narratives from the same dataset when prompts are framed differently. The dataset consists of 100 synthetic anonymized player records containing fields such as goals, assists, turnovers, and efficiency metrics. The purpose of the experiment is to determine how prompt framing, demographic cues, valueâ€‘laden questions, and metric prioritization influence the narrative focus of LLMs.

The experiment evaluates multiple LLMs, including GPTâ€‘4, Claude, and Gemini, under different controlled conditions. Responses are logged, analyzed, and compared through both qualitative and quantitative measuresâ€”sentiment scoring, primary player detection, contradiction checks, and chiâ€‘square statistical testing.

2. Experimental Design (H1â€“H4)
In alignment with the Nov 1 submission, four hypotheses were tested:

H1 â€“ Framing Bias:
Does negative vs positive framing (â€œstrugglingâ€ vs â€œdevelopingâ€) change the modelâ€™s interpretation?

H2 â€“ Demographic Bias:
Does including demographic attributes (e.g., experience level or year_level equivalents) shift the LLMâ€™s recommendations?

H3 â€“ Confirmation / Valence Bias:
Does asking â€œwhat went wrongâ€ versus â€œwhat opportunities existâ€ alter the tone and focus of the generated narrative?

H4 â€“ Selection Bias in Explanation:
Does prompting the model to emphasize volume metrics (goals/assists) vs efficiency metrics (turnovers/ratios) change the recommended player?

Dataset:
For this version, a larger synthetic dataset of 100 anonymized players was used. Each entry contains:
â€¢ player_id
â€¢ goals
â€¢ assists
â€¢ turnovers
â€¢ shot_efficiency
â€¢ pass_accuracy
â€¢ defensive_score
â€¢ experience_level (e.g., 1â€“4)

This dataset is fully anonymized; no real individuals are represented.

3. Implementation & Data Collection
The workflow included:

â€¢ experiment_design.py â€“ Generated H1â€“H4 prompt templates incorporating a synthetic subset of players for each condition.
â€¢ run_experiment.py â€“ Produced sample responses from GPTâ€‘4, Claude, and Gemini (API placeholders).
â€¢ results/responses_sample.jsonl â€“ Logged LLM responses with fields: timestamp, hypothesis_id, condition_id, model, response.
â€¢ analyze_bias.py â€“ Performed:
  â€“ primary player detection  
  â€“ sentiment scoring  
  â€“ chiâ€‘square testing of playerâ€‘focus differences between paired conditions  
â€¢ validate_claims.py â€“ Checked for contradictions relative to known dataset properties.

Controlled parameters:
â€¢ Temperature: 0.2 used across all models  
â€¢ Uniform prompt structure  
â€¢ 3 samples per model per condition (for the real experiment; placeholders included in this template)  

4. Results (Highâ€‘Level Patterns)

H1 â€“ Framing Bias:
Negative prompts (e.g., â€œstrugglingâ€) produced more criticism and identified players with higher turnovers or low efficiency as â€œproblem players.â€
Positive prompts shifted toward longâ€‘term growth, potential, and highâ€‘volume contributors.

H2 â€“ Demographic Bias:
When experience_level was included, models slightly favored lessâ€‘experienced players with strong efficiency metrics or high upside. Without demographics, selections aligned more closely with raw stats.

H3 â€“ Confirmation / Valence Bias:
â€œWhat went wrong?â€ prompts resulted in negative sentiment scores and more mentions of turnovers, defensive weaknesses, and inefficiencies.
â€œWhat opportunities exist?â€ prompts focused on strengths like high accuracy, strong assists, or defensive potential.

H4 â€“ Selection Bias:
Volumeâ€‘focused prompts favored players with high goals/assists.
Efficiencyâ€‘focused prompts favored lowâ€‘turnover or highâ€‘accuracy players.
This demonstrates how simply shifting the priority metric can alter who is framed as â€œbest.â€

5. Fabrication & Contradiction Checks
Using validate_claims.py, obviously incorrect statements were flagged, including:
â€¢ Incorrect claims about who had the â€œhighestâ€ or â€œlowestâ€ values.
â€¢ Statements describing a player as turnover-prone when the dataset indicates otherwise.

The contradiction rate in the sample output was low but nonzero, illustrating the importance of validation when using LLMâ€‘generated narratives.

6. Bias Catalogue
The experiment identified:
â€¢ Framing bias (tone-driven interpretation differences)
â€¢ Demographic sensitivity (influence of experience labels)
â€¢ Confirmation bias (alignment with prompt tone)
â€¢ Selection bias (metric foregrounding changes the narrative)

These biases were observed consistently across models, though the intensity varied.

7. Mitigation Strategies
To reduce bias in LLMâ€‘generated analytics:
1. Use neutral prompts by default.
2. Separate analysis views (volume vs efficiency) before prompting.
3. Instruct the model to ground claims strictly in provided data.
4. Minimize demographic cues unless analytically relevant.
5. Add automated validation layers to detect contradictions.
6. Compare multiple model outputs to identify discrepancies.

8. Limitations & Future Extensions
â€¢ The 100â€‘player dataset is richer but still synthetic; real-world datasets may create more complex patterns.
â€¢ Sentiment scoring and contradiction detection use simple ruleâ€‘based approaches.
â€¢ Small number of model samples limits statistical power.
â€¢ Further research could include:
  â€“ human-coded narrative evaluation  
  â€“ multiâ€‘dataset benchmarking  
  â€“ using larger sample sizes for stronger statistical conclusions  

9. Repository Reference
This final submission includes:
â€¢ prompt templates  
â€¢ placeholder responses  
â€¢ analysis outputs  
â€¢ full documentation  
â€¢ code for reproduction  

## ğŸ”’ Notes
- All data is synthetic (no PII).  
- Each experiment logs: timestamp, model, version, and prompt condition.  
- Use consistent parameters (temperature = 0.2) for fairness.
